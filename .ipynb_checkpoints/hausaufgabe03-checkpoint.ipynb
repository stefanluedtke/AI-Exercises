{
 "cells": [
  {
   "attachments": {
    "grafik.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDPs und Reinforcement Learning\n",
    "\n",
    "Betrachten Sie das folgende Reinforcement-Learning-Problem:\n",
    "\n",
    "![grafik.png](attachment:grafik.png)\n",
    "\n",
    "Es gibt 6 Zustände $s_0,\\dots,s_5$ und 4 Aktionen UpC, Up, Left, Right. Die Aktionen\n",
    "funktionieren wie folgt:\n",
    "* UpC: Der Agent geht hoch, außer in $s_4$ und $s_5$ , wo nichts passiert. Reward: -1\n",
    "* Right: Der Agent bewegt sich nach rechts, in $s_0$,$s_2$, $s_4$, mit Reward 0. In den anderen Zuständen passiert nichts, und der Reward ist -1.\n",
    "* Left: Der Agent bewegt sich nach links, in $s_1$, $s_3$ , $s_5$ , mit Reward 0. In $s_0$ passiert nichts, und der Reward ist -1. In $s_2$ ist der Reward -100, und er bleibt in $s_2$ . In $s_4$ ist der Reward 10, der Agent landet in $s_0$.\n",
    "* Up: Mit Wahrscheinlichkeit 0.8, wie upC, mit Reward 0. Mit Wahrscheinlichkeit 0.1, gehe nach links, und mit Wahrscheinlichkeit 0.1 nach rechts (mit dem Reward, den die jeweilige Left oder Right Aktion hätte)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Ihre Namen hier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bewertung\n",
    "(wird von mir ausgefüllt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgaben\n",
    "\n",
    "* Implementieren Sie eine Funktion `reward`, die den Reward für einen Zustand `s`, eine Aktion `a` und einen Nachfolge-Zustand `s'` ausrechnet. Die Reward-Funktion soll wie oben angegeben gestaltet sein. Die Zustände sollen als natürliche Zahlen 0 bis 5 modelliert werden, die Aktionen als Paare von Strings: Der erste String soll angeben, welche Aktion ausgewählt wurde, der zweite String, welche Aktion tatsächlich ausgeführt wurde (gewählte Aktionen: `right`, `left` und `upC` und `up`, ausgeführte Aktionen: `right`, `left` und `upC`). `5 Punkte`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(s,a,snew):\n",
    "    #TODO\n",
    "    return None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implementieren Sie die Funktion `performAction`, die für einen Zustand `s` und eine *gewählte* Aktion `a`, die tatsächlich ausgeführte Aktion, und den Nachfolge-Zustand berechnet und als Paar zurückgibt. Das bedeutet, wenn als gewählte Aktion `up` übergeben wird, soll `upC`, `left` oder `right` entsprechend der oben beschriebenen Wahrscheinlichkeiten ausgeführt werden.  `5 Punkte`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performAction(s,a1):\n",
    "    #TODO\n",
    "    return None\n",
    "\n",
    "#Test: \n",
    "performAction(0,\"up\") #sollte entweder (\"upC\",2) oder (\"left\",0) oder (\"right\",1) zurückliefern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der folgende Code erzeugt eine zufällige Sequenz von Aktionen und daraus resultierenden Zuständen. Diese Sequenz soll später verwendet werden, um ihren Q-Learning-Algorithmus zu testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#gleichverteilt 100 Aktionen auswählen\n",
    "a1s = np.random.choice(np.array([\"up\",\"upC\",\"left\",\"right\"]),size=100,replace=True)\n",
    "\n",
    "a1s\n",
    "states = [0]\n",
    "actions = []\n",
    "\n",
    "#Berechne und speichere tatsächlich ausgeführte Aktionen, und Zustands-Folge\n",
    "for i in range(len(a1s)):\n",
    "    sanew = performAction(states[i],a1s[i])\n",
    "    actions = actions + [(a1s[i],sanew[0])]\n",
    "    states = states + [sanew[1]]\n",
    "\n",
    "print(states)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implementieren Sie die Funktion `qLearn`, die für eine Sequenz von Aktionen und Zuständen (so wie oben erzeugt), eine Q-Matrix durch Q-Learning berechnet. Die initiale Q-Matrix soll nur aus Nullen bestehen. Benutzen Sie folgende Formel (diese resultiert aus der Standardregel für die Lernrate $\\alpha = 1$): \n",
    "$Q(s,a) = r_{s,a} + \\gamma \\ max_{a'}(Q(a(s),a')$ (wobei $a(s)$ den Nachfolge-Zustand von $s$, für die Aktion $a$, bezeichnet). `10 Punkte`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearn(actions,states,gamma=0.5):\n",
    "    #TODO\n",
    "    return None\n",
    "Q = qLearn(actions,states)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Betrachten Sie die resultierende Q-Matrix. Hat ihr Algorithmus eine intuitiv sinnvolle Policy gelernt? `2 Punkte`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Schreiben Sie ihre Antwort hier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:exercises]",
   "language": "python",
   "name": "conda-env-exercises-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
