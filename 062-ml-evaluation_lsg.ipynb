{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5633e906",
   "metadata": {},
   "source": [
    "# Evaluation von Klassifikatoren\n",
    "\n",
    " Häufig möchte man eine feste Aufteilung in Trainings- und Validierungsdaten vermeiden (die Validierungs-Performance soll nicht von der Wahl der Daten-Aufteilung abhängen). Außerdem möchte man möglichst viele Daten zum Training benutzen. Diese Ziele lasssen sich mit einer Kreuzvalidierung erreichen:\n",
    "Dabei werden die Daten $S$ in die Teilmengen $S_i,\\dots,S_n$ aufgeteilt. In Lauf $i$ wird $S \\setminus S_i$ als Trainingsdaten und $S_i$ als Validierungsdaten verwendet. Die Klassifikationsgüte ist die durchschnittliche Klassifikationsgüte jedes Laufs. \n",
    " \n",
    "## Aufgabe 1\n",
    "* Implementieren Sie die Funktion `cv(clf,features,classes,n)`, die eine n-fache Kreuzvalidierung mit dem jeweiligen Klassifikator durchführt und die mittlere Accuracy zurückgibt. \n",
    "* Klassifizieren Sie den Pima-Datensatz. Verwenden Sie dazu einen Entscheidungsbaum. Welcher Klassifikator liefert das bessere Ergebnis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0c24825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6932765695993837\n",
      "0.768390781796271\n",
      "0.7390485142346825\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "def cv(clf,features,classes,n):\n",
    "    folds = np.random.choice(np.arange(n),features.shape[0],replace=True)\n",
    "    acc = np.arange(n).astype(\"double\")\n",
    "    for i in range(n):\n",
    "        train = features.loc[folds != i,:]\n",
    "        ytrain = classes.loc[folds != i].astype(\"str\")\n",
    "        clf.fit(train,ytrain)\n",
    "        \n",
    "        val = features.loc[folds == i,:]\n",
    "        yval = classes.loc[folds == i].astype(\"str\")\n",
    "        \n",
    "        p = clf.predict(val)\n",
    "        acc[i]=sum(p == yval)/len(p)\n",
    "    return np.mean(acc)\n",
    "\n",
    "dataset = pd.DataFrame(loadtxt('pima-indians-diabetes.csv', delimiter=','))\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset.iloc[:,0:8]\n",
    "y = dataset.iloc[:,8]\n",
    "\n",
    "acctree = cv(tree.DecisionTreeClassifier(),X,y,10)\n",
    "acclda = cv(LinearDiscriminantAnalysis(),X,y,10)\n",
    "accqda = cv(QuadraticDiscriminantAnalysis(),X,y,10)\n",
    "print(acctree)\n",
    "print(acclda)\n",
    "print(accqda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438fdbdb",
   "metadata": {},
   "source": [
    "## Aufgabe 2\n",
    "In Scipy sind verschiedene Klassifikatoren bereits implementiert. Im Folgenden sollen die Klassifikatoren miteinander verglichen werden. Vergleichen Sie die folgenden Klassifikatoren mit 10-fold CV anhand der Accuracy:  Decision Trees, Lineare Diskriminantenanalyse, Quadratische Diskriminantenanalyse, Support Vector Machine, sowie das neuronale Netz aus Übung 6.1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c48b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tt = round(3/4 * df.shape[0])\n",
    "train = df.iloc[range(0,tt),:]\n",
    "test = df.iloc[range(tt,df.shape[0])]\n",
    "\n",
    "y = train[\"class\"].astype(\"str\")\n",
    "y_test = test[\"class\"].astype(\"str\")\n",
    "\n",
    "classfs = [DecisionTreeClassifier(),LinearDiscriminantAnalysis(),QuadraticDiscriminantAnalysis(),LinearSVC()]\n",
    "def getAcc(clf):\n",
    "    clf = clf.fit(train.iloc[:,range(0,12)],y)\n",
    "    y_pred = clf.predict(test.iloc[:,range(0,12)])\n",
    "    cm = cfm(y_test,y_pred)\n",
    "    plt.matshow(cm,cmap='hot')\n",
    "    \n",
    "    return (sum(y_test == y_pred)/len(y_pred))\n",
    "\n",
    "accs = [getAcc(clf) for clf in classfs]\n",
    "accs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
